from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from config import Config
from .tree import Tree
from .node import Node

from arc_rstar.tools.python_tool import extract_python_code, execute_code_with_grid


class BeamSearch(Tree):
    """
    Beam Search implementation for ARC tasks.
    
    The search uses two key parameters:
    - branching_factor: Number of candidates to generate at each node (exploration breadth)
    - beam_width: Number of "best candidates" to keep after each step (pruning)
    
    This separation allows controlling generation diversity and search efficiency independently.
    """
    def __init__(self, config: Config):
        super().__init__(config)
        self.beam_width = config.beam_width
        self.branching_factor = config.branching_factor
        self.max_depth = config.max_depth
        self.best_node = None
        self.best_score = float('-inf')
        
    def search(self, task, policy_model) -> Node:
        """
        Execute beam search and return the best node found.
        
        Args:
            task: The task to solve
            policy_model: The policy model to generate candidates
            
        Returns:
            The best node found during search
        """
        if self.root is None:
            # Initialize the root with the task prompt
            initial_state = {"text": task.get_initial_prompt(), "extra_info": ""}
            self.initialize_root(initial_state)
        
        # Initialize beam with just the root node
        beam = [self.root]
        
        for depth in range(self.max_depth):
            if not beam:
                break
            
            # Generate candidate next steps for all nodes in the current beam
            candidates = []
            
            for node in beam:
                if node.is_terminal:
                    # Keep terminal nodes in the candidates
                    candidates.append((node.score, node))
                    continue
                
                # Generate potential next steps using the policy model
                next_states = self._generate_candidates(node, task, policy_model)
                
                # Add all generated candidates to the pool
                for state, score in next_states:
                    child = self.add_child(node, state)
                    child.set_score(score)
                    candidates.append((score, child))
                    
                    # Update "best_node" if this is better
                    if score > self.best_score:
                        self.best_score = score
                        self.best_node = child

            sorted_candidates = sorted(candidates, key=lambda x: -x[0])
            
            # Select top-k candidates for the next beam
            beam = [node for _, node in sorted_candidates[:self.beam_width]]
            
            # Check if we've found a solution
            for node in beam:
                if task.is_solved(node.state):
                    node.is_terminal = True
                    return node
        
        # Return the best node found
        return self.best_node if self.best_node else self.root
    
    def _generate_candidates(self, node: Node, task, policy_model) -> List[Tuple[Dict[str, Any], float]]:
        """
        Generate candidate next steps using the policy model.
        Focusing on generating Python code to solve the ARC task.
        
        Args:
            node: Current node to expand
            task: The task being solved
            policy_model: The language model to generate candidates
            
        Returns:
            List of (state, score) tuples for candidate next steps
        """

        # Get the current state/prompt
        prompt = node.state["text"]
        
        # Generate next steps using the policy model
        candidates = []
        
        # Generate completions using the policy model
        completions = policy_model.generate(prompt)

        # Process completions into candidate states
        for i, completion in enumerate(completions):
            # Extract the generated text
            generated_text = completion.outputs[0].text
            
            # Create a new state by extending the current prompt
            new_text = prompt + generated_text
            new_state = {
                "text": new_text,
                "extra_info": "Generated by beam search step"
            }
            
            # Extract Python code and evaluate it if possible
            code = extract_python_code(new_text)
            score = -i  # Default score based on order
            
            if code and hasattr(task, 'test_data') and task.test_data:
                # Try to execute the code
                test_input = task.test_data[0].input_grid.grid
                result = execute_code_with_grid(code, test_input)
                
                # Score based on code execution success
                if result.get('success', False):
                    # If code executes successfully, give it a higher score
                    score = 10.0
                    
                    # If there's an output grid, check if it's close to expected
                    if 'grid' in result:
                        # Check if dimensions match the expected output
                        expected_output = task.test_data[0].output_grid.grid
                        output_grid = result.get('grid')
                        
                        # Convert to list of lists if it's a numpy array
                        if isinstance(output_grid, np.ndarray):
                            output_grid = output_grid.tolist()
                        
                        # Further boost score for correct answers
                        if output_grid == expected_output:
                            score = 100.0  # Much higher score for correct answers
                        elif len(output_grid) == len(expected_output):
                            # Partial match - dimensions match
                            score = 20.0
            
            # Add the score to the state for reference
            new_state["score"] = score
            candidates.append((new_state, score))
        
        return candidates

