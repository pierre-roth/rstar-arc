log-level: INFO
round-number: 5

task-validation-fraction: 0.002
example-validation-num: 1
example-validation-threshold: 5
example-validation-probability: 0.01

max-seq-len: 14336

resume-from-checkpoint: false

fine_tuned: true
policy-model: "fine_tuned_1.5B_1_epoch"
training-dataset-name: "policy_dataset_training.jsonl"
validation-dataset-name: "policy_dataset_validation.jsonl"
use-bf16: true
attn-implementation: "flash_attention_2"
torch-compile: false

full-finetune: true

train-on-prompts: false

learning-rate: 0.000009
weight_decay: 0

per-device-train-batch-size: 1
per-device-eval-batch-size: 1
gradient-accumulation-steps: 32
num-train-epochs: 1
lr-scheduler-type: "cosine"
warmup-ratio: 0.1
logging-steps: 125
eval-steps: 250
save-steps: 250
save-total-limit: 3
gradient-checkpointing: true

report-to: "wandb"