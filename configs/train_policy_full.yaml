log-level: INFO
round-number: 2

max-seq-len: 20480

policy-model: "Qwen/Qwen2.5-Coder-1.5B"
use-bf16: true

full-finetune: true
weight_decay: 0.01

learning-rate: 0.00004

per-device-train-batch-size: 1
per-device-eval-batch-size: 1
gradient-accumulation-steps: 16
num-train-epochs: 1
lr-scheduler-type: "cosine"
warmup-ratio: 0.03
logging-steps: 25
eval-steps: 50
save-steps: 50
save-total-limit: 3
gradient-checkpointing: true

report-to: "wandb"

